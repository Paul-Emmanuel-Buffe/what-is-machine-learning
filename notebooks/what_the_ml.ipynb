{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ee7336-e6b9-453c-8fde-9d9830894659",
   "metadata": {},
   "source": [
    "# What the ML\n",
    "\n",
    "**Découvrez comment le Machine Learning révolutionne notre façon de penser et de créer.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Le Machine Learning (ou apprentissage automatique) est une composante essentielle de l'intelligence artificielle. Il permet aux machines d'apprendre à partir de données sans programmation explicite, en identifiant des motifs et en fournissant des prédictions précises.  \n",
    "Avant de commencer les projets pratiques, il est important de comprendre les notions clés suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbfdd6f-0642-414c-b543-902c93f4292d",
   "metadata": {},
   "source": [
    "## A. La science des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b43b9-a58a-4369-b372-6d9da99aacde",
   "metadata": {},
   "source": [
    "## B. L’apprentissage automatique et/ou l’apprentissage profond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca8bbc-6353-4c28-bb16-e448d659c139",
   "metadata": {},
   "source": [
    "## C. L’apprentissage supervisé,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152cbd3-8ecf-4cc6-8552-2e586a923022",
   "metadata": {},
   "source": [
    "## D. L'Apprentissage Non Supervisé\n",
    "\n",
    "### Introduction\n",
    "\n",
    "L'apprentissage non supervisé est une branche du machine learning qui permet d'analyser et de structurer des données sans étiquetage préalable. Cette approche est particulièrement utile pour découvrir des structures cachées dans de grandes quantités de données. Les modèles d'apprentissage non supervisé sont employés pour trois tâches principales : **le clustering (partitionnement)**, **l'association** et **la réduction de la dimensionnalité**.\n",
    "\n",
    "---\n",
    "\n",
    "### I. Le Partitionnement (Clustering)\n",
    "\n",
    "Le partitionnement regroupe des données non étiquetées selon leurs similitudes ou différences. Cette technique transforme des objets de données bruts en structures organisées représentant l'information sous-jacente.\n",
    "\n",
    "#### 1. Partitionnement Exclusif : K-moyennes\n",
    "\n",
    "Le partitionnement en **K-moyennes** affecte chaque point de données à l'un des K groupes prédéfinis. Les points sont assignés au cluster dont le centroïde (centre) est le plus proche. Les données les plus proches d'un même centroïde forment ainsi une catégorie homogène.\n",
    "\n",
    "#### 2. Partitionnement Hiérarchique\n",
    "\n",
    "Le partitionnement hiérarchique se décline en deux approches :\n",
    "\n",
    "**Approche agglomérative (ascendante)** : Chaque point de données commence dans un cluster distinct, puis les clusters similaires fusionnent progressivement jusqu'à n'en former qu'un seul. La distance euclidienne mesure généralement la similarité via quatre méthodes :\n",
    "- **Liaison de Ward** : basée sur l'augmentation de la somme des carrés après fusion\n",
    "- **Liaison moyenne** : distance moyenne entre tous les points de deux clusters\n",
    "- **Liaison complète** : distance maximale entre deux points de clusters différents\n",
    "- **Liaison simple** : distance minimale entre deux points de clusters différents\n",
    "\n",
    "**Approche divisive (descendante)** : Un seul cluster initial est progressivement divisé selon les différences entre points. Cette méthode est moins couramment utilisée.\n",
    "\n",
    "#### 3. Partitionnement Probabiliste\n",
    "\n",
    "Cette approche regroupe les points selon la **probabilité** qu'ils appartiennent à une distribution particulière. Le **Modèle de Mélange Gaussien (GMM)** est la méthode probabiliste la plus répandue, permettant un partitionnement \"doux\" où un point peut appartenir partiellement à plusieurs clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### II. Les Règles d'Association\n",
    "\n",
    "Les règles d'association identifient des relations entre variables dans un jeu de données.\n",
    "\n",
    "#### Algorithmes Apriori\n",
    "\n",
    "Issus de l'analyse de panier d'achat, les algorithmes Apriori alimentent aujourd'hui les moteurs de recommandation. Ils identifient des ensembles d'éléments fréquemment associés pour prédire la consommation d'un produit en fonction d'un autre. \n",
    "\n",
    "**Exemple concret** : Sur Spotify, écouter Black Sabbath déclenche des recommandations de Led Zeppelin, basées sur les habitudes d'écoute personnelles et collectives. Ces algorithmes utilisent un arbre de hachage pour analyser exhaustivement les données transactionnelles.\n",
    "\n",
    "---\n",
    "\n",
    "### III. La Réduction de la Dimensionnalité\n",
    "\n",
    "Bien que davantage de données améliorent généralement la précision, elles peuvent provoquer du surajustement et compliquer la visualisation. La **réduction de la dimensionnalité** diminue le nombre de caractéristiques tout en préservant l'intégrité des données. Cette étape de prétraitement est essentielle pour gérer des données complexes.\n",
    "\n",
    "#### 1. Analyse en Composantes Principales (ACP)\n",
    "\n",
    "L'ACP simplifie des données complexes en créant de nouvelles variables synthétiques appelées \"composantes principales\". \n",
    "\n",
    "**Principe** : La première composante capture la direction de variance maximale. La deuxième composante cherche aussi la variance maximale, mais dans une direction perpendiculaire à la première. Ce processus se répète, chaque nouvelle composante étant orthogonale aux précédentes. Résultat : l'information essentielle est conservée avec moins de dimensions.\n",
    "\n",
    "#### 2. Décomposition en Valeurs Singulières (SVD)\n",
    "\n",
    "La SVD factorise une matrice complexe A en trois matrices plus simples (A = USVT), où U et V sont des matrices orthogonales et S est une matrice diagonale contenant les valeurs singulières. Comme l'ACP, elle sert à réduire le bruit et compresser les données, notamment les fichiers image.\n",
    "\n",
    "#### 3. Auto-encodeurs\n",
    "\n",
    "Les auto-encodeurs utilisent des réseaux de neurones pour compresser puis reconstruire les données. La **couche cachée** agit comme un goulet d'étranglement : elle compresse l'information (phase d'**encodage**) avant de la reconstruire dans la couche de sortie (phase de **décodage**).\n",
    "\n",
    "---\n",
    "\n",
    "### IV. Applications Concrètes\n",
    "\n",
    "L'apprentissage non supervisé améliore l'expérience utilisateur et identifie rapidement des modèles dans de grandes quantités de données :\n",
    "\n",
    "- **Rubriques d'actualités** : Google Actualités regroupe automatiquement des articles similaires provenant de sources différentes\n",
    "- **Vision par ordinateur** : reconnaissance d'objets dans les images\n",
    "- **Imagerie médicale** : détection, classification et segmentation d'images pour des diagnostics rapides et précis\n",
    "- **Détection d'anomalies** : identification de données atypiques révélant des équipements défectueux, erreurs humaines ou failles de sécurité\n",
    "- **Profils clients** : identification des traits communs et habitudes d'achat pour personnaliser les messages marketing\n",
    "- **Moteurs de recommandation** : analyse des comportements de consommation pour suggérer des produits complémentaires pertinents\n",
    "\n",
    "---\n",
    "\n",
    "### V. Les Défis de l'Apprentissage Non Supervisé\n",
    "\n",
    "Malgré ses avantages, cette approche présente plusieurs limitations :\n",
    "\n",
    "- **Complexité informatique** due au volume élevé des données d'entraînement\n",
    "- **Temps d'entraînement prolongés**\n",
    "- **Risque accru de résultats inexacts**\n",
    "- **Intervention humaine nécessaire** pour valider les variables de sortie\n",
    "- **Manque de transparence** quant aux critères de regroupement des données\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "L'apprentissage non supervisé offre des outils puissants pour explorer et structurer de grandes quantités de données sans étiquetage préalable. Du partitionnement à la réduction de dimensionnalité, ces techniques transforment des données brutes en informations exploitables, avec des applications concrètes dans de nombreux domaines. Toutefois, leur mise en œuvre requiert une validation rigoureuse et une compréhension approfondie de leurs limites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042050a-18ed-4f06-bbcb-23f72bde3ced",
   "metadata": {},
   "source": [
    "## E. La classification supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8490f08-1ff7-4aab-abec-d6221d39a5e1",
   "metadata": {},
   "source": [
    "## F. La Classification Non Supervisée\n",
    "\n",
    "### Introduction\n",
    "\n",
    "La classification non supervisée désigne le processus de construction de classes à partir de données non étiquetées, indépendamment de tout objectif prédictif explicite. Contrairement à l’apprentissage non supervisé, qui regroupe plusieurs familles de méthodes, la classification non supervisée se concentre sur le résultat analytique : l’organisation des données en catégories homogènes révélant leur structure interne.\n",
    "\n",
    "---\n",
    "\n",
    "### I. Objectifs de la Classification Non Supervisée\n",
    "\n",
    "La classification non supervisée vise principalement à :\n",
    "\n",
    "- structurer un ensemble de données complexes ou volumineuses ;\n",
    "- identifier des profils ou comportements similaires ;\n",
    "- révéler des régularités ou des patterns latents ;\n",
    "- fournir une base analytique à des décisions métier ou à des analyses ultérieures.\n",
    "\n",
    "Elle s’inscrit dans une démarche exploratoire et descriptive, sans finalité prédictive directe.\n",
    "\n",
    "---\n",
    "\n",
    "### II. Fondements Méthodologiques\n",
    "\n",
    "La classification non supervisée repose sur trois éléments essentiels :\n",
    "\n",
    "- la sélection et la préparation des variables décrivant les individus ;\n",
    "- le choix d’une mesure de similarité ou de distance ;\n",
    "- une règle de regroupement permettant d’affecter les observations à des classes.\n",
    "\n",
    "Les classes sont construites de manière à maximiser l’homogénéité interne tout en assurant une différenciation claire entre les groupes.\n",
    "\n",
    "---\n",
    "\n",
    "### III. Caractère Non Unique de la Classification\n",
    "\n",
    "Pour un même jeu de données, plusieurs classifications non supervisées peuvent être produites. Les résultats dépendent fortement :\n",
    "\n",
    "- des variables retenues et de leur transformation ;\n",
    "- des méthodes de normalisation appliquées ;\n",
    "- des paramètres et hypothèses propres aux algorithmes utilisés.\n",
    "\n",
    "Il n’existe donc pas de classification unique ou universelle, mais différentes représentations possibles de la structure des données.\n",
    "\n",
    "---\n",
    "\n",
    "### IV. Interprétation et Validation des Classes\n",
    "\n",
    "En l’absence de classes de référence, l’interprétation constitue une étape centrale du processus. Elle implique :\n",
    "\n",
    "- l’analyse des caractéristiques dominantes de chaque classe ;\n",
    "- la comparaison des classes entre elles ;\n",
    "- la validation de leur cohérence au regard du contexte d’étude.\n",
    "\n",
    "Cette phase introduit une supervision humaine a posteriori, indispensable pour donner du sens aux résultats obtenus.\n",
    "\n",
    "---\n",
    "\n",
    "### V. Rôle dans un Pipeline Data\n",
    "\n",
    "La classification non supervisée est fréquemment utilisée comme :\n",
    "\n",
    "- étape exploratoire initiale ;\n",
    "- outil de segmentation descriptive ;\n",
    "- support à la création de variables synthétiques ;\n",
    "- aide à la prise de décision stratégique.\n",
    "\n",
    "Elle s’intègre généralement en amont d’analyses statistiques avancées ou de modèles supervisés.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "La classification non supervisée constitue un levier essentiel de l’analyse exploratoire des données. En structurant l’information sans référence préalable, elle permet de mieux comprendre des ensembles de données complexes. Toutefois, son efficacité repose sur des choix méthodologiques rigoureux et sur la capacité de l’analyste à interpréter et valider les classes produites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4fd7a7-8127-471d-9277-dc6910ac9b02",
   "metadata": {},
   "source": [
    "## G. La régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95023a7-338a-4a62-812c-acf6a1005d81",
   "metadata": {},
   "source": [
    "## H. Validation Croisée (Cross-Validation)\n",
    "\n",
    "### Définition\n",
    "\n",
    "La validation croisée est une méthode permettant de tester les performances d'un modèle prédictif de Machine Learning. Elle consiste à évaluer si les résultats numériques quantifiant les relations entre variables sont acceptables comme descriptions des données.\n",
    "\n",
    "**Objectifs principaux** :\n",
    "- Comparer différents modèles et sélectionner le plus approprié\n",
    "- Évaluer un modèle même avec des données limitées\n",
    "- Éviter le sur-ajustement et le biais\n",
    "\n",
    "### Principe Général\n",
    "\n",
    "La validation croisée est une procédure de ré-échantillonnage. Une partie des données est écartée en amont du dataset d'entraînement. Ces données ne serviront pas à entraîner le modèle, mais à le tester et le valider ultérieurement.\n",
    "\n",
    "---\n",
    "\n",
    "## I. Train-Test Split\n",
    "\n",
    "### Principe\n",
    "\n",
    "L'approche Train-Test Split décompose de manière aléatoire un ensemble de données en deux parties :\n",
    "- **Ensemble d'entraînement** : pour entraîner le modèle\n",
    "- **Ensemble de test** : pour valider le modèle\n",
    "\n",
    "### Avantages et Limites\n",
    "\n",
    "**Avantages** :\n",
    "- Simple et rapide à implémenter\n",
    "- Disponible dans scikit-learn (méthode `train_test_split`)\n",
    "\n",
    "**Limites** :\n",
    "- Problématique avec des données limitées\n",
    "- Risque de manquer des informations non utilisées pour l'entraînement\n",
    "- Résultats potentiellement biaisés\n",
    "\n",
    "---\n",
    "\n",
    "## II. K-Folds Cross-Validation\n",
    "\n",
    "### Principe\n",
    "\n",
    "La méthode K-Folds assure que toutes les observations du dataset original apparaissent dans l'ensemble d'entraînement ET dans l'ensemble de test. C'est l'une des meilleures approches en cas de données limitées.\n",
    "\n",
    "### Fonctionnement\n",
    "\n",
    "1. **Séparation** : Le dataset est divisé aléatoirement en K parties (folds)\n",
    "2. **Entraînement** : Le modèle est ajusté avec K-1 folds\n",
    "3. **Validation** : Le fold restant sert à valider le modèle\n",
    "4. **Répétition** : Le processus est répété jusqu'à ce que chaque fold ait servi de validation\n",
    "5. **Score final** : La moyenne des scores enregistrés constitue la métrique de performance\n",
    "\n",
    "### Choix de K\n",
    "\n",
    "- **Valeur recommandée** : Entre 5 et 10 selon l'envergure du dataset\n",
    "- **K élevé** : Modèle moins biaisé, mais risque de variance élevée et sur-ajustement\n",
    "- **K faible** : Équivaut à un Train-Test Split classique\n",
    "\n",
    "---\n",
    "\n",
    "## III. Stratified K-Fold\n",
    "\n",
    "### Contexte d'Usage\n",
    "\n",
    "Utilisée par défaut lorsque :\n",
    "- Le modèle est un classificateur\n",
    "- La variable cible est binaire ou multiclasse\n",
    "\n",
    "### Principe\n",
    "\n",
    "La méthode Stratified K-Fold maintient le pourcentage d'échantillons pour chaque classe dans tous les folds. Les données des folds d'entraînement et de test sont ainsi équitablement distribuées.\n",
    "\n",
    "**Exemple** : Si le dataset contient 30% de classe A et 70% de classe B, chaque fold respectera cette répartition 30/70.\n",
    "\n",
    "---\n",
    "\n",
    "## IV. Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "### Définition\n",
    "\n",
    "La validation croisée \"un contre tous\" est un cas particulier de la validation croisée à k blocs avec **k = n** (n = nombre total d'observations).\n",
    "\n",
    "### Fonctionnement\n",
    "\n",
    "À chaque itération :\n",
    "- **Entraînement** : sur n-1 observations\n",
    "- **Validation** : sur l'unique observation restante\n",
    "\n",
    "Le modèle est donc entraîné et validé n fois (une fois par observation).\n",
    "\n",
    "### Usage\n",
    "\n",
    "Particulièrement utile pour des datasets très petits, mais coûteux en temps de calcul pour des datasets volumineux.\n",
    "\n",
    "---\n",
    "\n",
    "## V. Gestion des Bases Déséquilibrées\n",
    "\n",
    "### Problématique\n",
    "\n",
    "Dans les tâches de classification, la répartition des classes peut être déséquilibrée : le nombre d'observations n'est pas le même d'une classe à l'autre.\n",
    "\n",
    "**Exemple** : 3 observations de classe 1 pour 7 observations de classe 2.\n",
    "\n",
    "### Solution : Validation Croisée Stratifiée\n",
    "\n",
    "La **stratification** s'assure que la répartition des classes soit identique dans tous les ensembles d'apprentissage et de validation.\n",
    "\n",
    "**Principe** :\n",
    "- Si le dataset initial présente un ratio 3:7 entre deux classes\n",
    "- Chaque ensemble de validation et d'apprentissage devra présenter ce même ratio 3:7\n",
    "\n",
    "**Avantages** :\n",
    "- Évite le biais lié à une répartition changeante des classes\n",
    "- Garantit une performance de validation représentative\n",
    "\n",
    "---\n",
    "\n",
    "## VI. Processus Complet de Validation\n",
    "\n",
    "### Étapes\n",
    "\n",
    "1. **Séparation initiale** : Mettre de côté un jeu de test (données jamais vues)\n",
    "2. **Validation croisée** : Sur les données d'entraînement restantes\n",
    "3. **Sélection du modèle** : Choisir le meilleur modèle selon les scores CV\n",
    "4. **Test final** : Évaluer le modèle sélectionné sur le jeu de test mis de côté\n",
    "\n",
    "### Résumé\n",
    "```\n",
    "Dataset complet\n",
    "    ↓\n",
    "[Jeu de test mis de côté] ← Ne pas toucher jusqu'au test final\n",
    "    ↓\n",
    "Données d'entraînement\n",
    "    ↓\n",
    "K-Folds Cross-Validation\n",
    "    ↓\n",
    "Sélection du meilleur modèle\n",
    "    ↓\n",
    "Test final sur le jeu de test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "La validation croisée est essentielle pour évaluer objectivement les performances d'un modèle. Le choix de la méthode (Train-Test Split, K-Folds, Stratified K-Folds, LOOCV) dépend de la taille du dataset, de l'équilibre des classes et des ressources de calcul disponibles. Une validation rigoureuse garantit des modèles robustes et généralisables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd0e01-5219-49d8-9aed-0415f801ed7f",
   "metadata": {},
   "source": [
    "## I. Les données d’entraînement, les données de test et/ ou de validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1573754-203a-483c-bbd0-f745c221d74a",
   "metadata": {},
   "source": [
    "## J. Corrélation linéaire (de Pearson) entre deux variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d230b-c8ee-45ac-bc4d-57cf738a8916",
   "metadata": {},
   "source": [
    "## K. Une fonction de coût"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b575b-3ab6-4932-a726-d25cb62651fe",
   "metadata": {},
   "source": [
    "## La descente de gradient\n",
    "\n",
    "La descente de gradient est un algorithme d’optimisation couramment utilisé pour entraîner les modèles de machine learning. Son objectif est de réduire l’erreur entre les valeurs prédites par le modèle et les valeurs réelles.\n",
    "\n",
    "Lors de l’entraînement, le modèle ajuste progressivement ses paramètres à partir des données d’apprentissage. La fonction de coût agit comme un indicateur de performance en mesurant l’erreur à chaque itération. Tant que cette erreur n’est pas proche de zéro, le modèle continue d’apprendre.\n",
    "\n",
    "La descente de gradient fonctionne de manière similaire à la recherche d’une droite de meilleur ajustement, en minimisant l’erreur quadratique moyenne entre les valeurs réelles et les valeurs prédites. Elle repose généralement sur une fonction convexe, facilitant la convergence.\n",
    "\n",
    "Objectif : réduire la fonction de coût, c’est-à-dire l’erreur entre y réel et y prédit.\n",
    "\n",
    "Les calculs successifs de dérivées partielles permettent à l’algorithme de se rapprocher progressivement du minimum local ou global, appelé point de convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Taux d’apprentissage (alpha)\n",
    "\n",
    "Le taux d’apprentissage, noté alpha, correspond à la taille des pas effectués pour atteindre le minimum de la fonction de coût.\n",
    "\n",
    "Un taux élevé permet une convergence plus rapide, mais comporte un risque de dépassement du minimum.\n",
    "Un taux faible améliore la précision, mais rend l’apprentissage plus lent et plus coûteux en calcul.\n",
    "\n",
    "---\n",
    "\n",
    "## Fonction de coût (ou fonction de perte)\n",
    "\n",
    "La fonction de coût mesure l’écart entre les valeurs réelles et les valeurs prédites. Elle guide l’ajustement des paramètres du modèle afin de minimiser l’erreur.\n",
    "\n",
    "L’algorithme progresse à chaque itération dans la direction du gradient négatif jusqu’à convergence.\n",
    "\n",
    "La fonction de perte mesure l’erreur pour une seule observation, tandis que la fonction de coût correspond à la moyenne des erreurs sur l’ensemble des données d’entraînement.\n",
    "\n",
    "---\n",
    "\n",
    "## Fonction mathématique de la descente de gradient\n",
    "\n",
    "### Fonction à minimiser (fonction de coût)\n",
    "\n",
    "On cherche à mesurer l’erreur globale du modèle :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff52f9-af47-42e8-9595-f17c483630dd",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Avec :\n",
    "- $y_i$ : valeur réelle  \n",
    "- $\\hat{y}_i$ : valeur prédite par le modèle  \n",
    "- $n$ : nombre d’observations  \n",
    "- $J(\\theta)$ : erreur globale du modèle  \n",
    "\n",
    "Plus la valeur de $J(\\theta)$ est faible, plus le modèle est performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbf666-2cc7-4176-ae37-bd877f944182",
   "metadata": {},
   "source": [
    "### Principe de la descente de gradient\n",
    "\n",
    "L’objectif est de modifier progressivement les paramètres du modèle, notés $\\theta$, afin de réduire l’erreur.  \n",
    "Pour cela, on utilise la pente de la fonction de coût, appelée gradient.\n",
    "\n",
    "---\n",
    "\n",
    "### Règle de mise à jour des paramètres\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Dans cette équation :\n",
    "- $\\theta$ représente les paramètres du modèle  \n",
    "- $\\alpha$ est le taux d’apprentissage  \n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ indique la direction de la plus forte augmentation de l’erreur  \n",
    "\n",
    "La soustraction du gradient permet de se déplacer vers le minimum de la fonction de coût.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition visuelle\n",
    "\n",
    "La descente de gradient peut être comparée à une marche sur une colline. Le point de départ correspond à une erreur élevée, et chaque pas, guidé par la pente, rapproche progressivement du point le plus bas où l’erreur est minimale.\n",
    "\n",
    "---\n",
    "\n",
    "### Condition d’arrêt\n",
    "\n",
    "L’algorithme s’arrête lorsque l’erreur devient très faible ou lorsque les mises à jour des paramètres deviennent négligeables.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
